import { supabase } from '@/lib/supabaseClient'
import { getSingleEmbedding } from './deepseekEmbedding'
import { splitAndEmbedSentences } from './splitAndEmbedSentences'

interface UploadContent {
  userId: string
  work: any[]
  project: any[]
  education: any[]
  award: any[]
  skills: string[]
}

// âœ… å»é‡å·¥å…·ï¼šå¥å­çº§æ–‡æœ¬å»é‡
function deduplicateSentences(text: string): string {
  const sentences = text
    .split(/(?<=[.ã€‚!?ï¼ï¼Ÿâ€â€\n])\s*/g)
    .map((s) => s.trim())
    .filter((s) => s.length > 0)

  const seen = new Set<string>()
  const unique = sentences.filter((s) => {
    if (seen.has(s)) return false
    seen.add(s)
    return true
  })

  return unique.join(' ')
}

export async function uploadEmbeddingsToSupabaseFromData(data: UploadContent) {
  const userId = data.userId

  // ğŸ§¹ åˆ é™¤å½“å‰ç”¨æˆ·æ¯ä¸ªç±»å‹çš„æ—§å‘é‡
  const contentTypes = ['work', 'project', 'education', 'award', 'skill']

  for (const type of contentTypes) {
    const { error: deleteError } = await supabase
      .from('cv_vector_data')
      .delete()
      .eq('user_id', userId)
      .eq('content_type', type)

    if (deleteError) {
      console.warn(`âš ï¸ åˆ é™¤ ${type} ç±»å‹æ—§å‘é‡å¤±è´¥:`, deleteError)
    } else {
      console.log(`ğŸ§¹ æ¸…ç†æˆåŠŸ: ${type} å‘é‡å·²åˆ é™¤`)
    }
  }

  const rows: { content_type: string; raw_text: string; user_id: string }[] = []

  data.work?.forEach((w) => {
    const raw = [w.company, w.title, w.responsibilities, w.achievements].filter(Boolean).join(' ')
    const clean = deduplicateSentences(raw)
    if (clean.trim()) rows.push({ content_type: 'work', raw_text: clean, user_id: userId })
  })

  data.project?.forEach((p) => {
    const raw = [p.title, p.description].filter(Boolean).join(' ')
    const clean = deduplicateSentences(raw)
    if (clean.trim()) rows.push({ content_type: 'project', raw_text: clean, user_id: userId })
  })

  data.education?.forEach((e) => {
    const raw = [e.school, e.degree, e.major, e.description].filter(Boolean).join(' ')
    const clean = deduplicateSentences(raw)
    if (clean.trim()) rows.push({ content_type: 'education', raw_text: clean, user_id: userId })
  })

  data.award?.forEach((a) => {
    const raw = [a.title, a.source, a.description].filter(Boolean).join(' ')
    const clean = deduplicateSentences(raw)
    if (clean.trim()) rows.push({ content_type: 'award', raw_text: clean, user_id: userId })
  })

  data.skills?.forEach((s) => {
    const trimmed = s.trim()
    if (trimmed) rows.push({ content_type: 'skill', raw_text: trimmed, user_id: userId })
  })

  const insertPayload = []

  for (const row of rows) {
    const embedding = await getSingleEmbedding(row.raw_text)
    if (!embedding) {
      console.warn('âš ï¸ æ— æ³•è·å– embedding:', row.raw_text)
      continue
    }

    const { sentences, embeddings } = await splitAndEmbedSentences(row.raw_text)

    insertPayload.push({
      user_id: row.user_id,
      content_type: row.content_type,
      raw_text: row.raw_text,
      embedding,
      sentence_embeddings: {
        sentences,
        embeddings,
      },
    })
  }

  if (insertPayload.length === 0) {
    console.warn('âš ï¸ æ— éœ€ä¸Šä¼ ï¼Œæœªæ‰¾åˆ°æœ‰æ•ˆæ–°è®°å½•')
    return
  }

  const { error } = await supabase.from('cv_vector_data').insert(insertPayload)
  if (error) throw new Error('âŒ å‘é‡ä¸Šä¼ å¤±è´¥: ' + JSON.stringify(error, null, 2))

  console.log(`âœ… æˆåŠŸä¸Šä¼  ${insertPayload.length} æ¡å‘é‡è®°å½•åˆ° Supabase`)
}
